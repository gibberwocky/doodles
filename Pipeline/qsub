#!/bin/bash
# IEKNGHER1ujm

# To view jobs:			qstat -u dwragg
# To query error state:		qstat -j <jobid>
# To delete a job:		qdel <jobid>


# ===========
# GENOME PREP
# ===========
#PICARD="/usr/local/bioinfo/src/picard-tools/current"
#REF="/home/dwragg/save/Genomes/Amel_4.5_scaffolds"
#bwa index -a bwtsw ${REF}.fa
#samtools faidx ${REF}.fa
#java -d64 -Xmx48g -jar ${PICARD}/CreateSequenceDictionary.jar \
#  REFERENCE=${REF}.fa \
#  OUTPUT=${REF}.dict


# Run_Pool1.4968/RawData	0-21

# Location of links to paired .fastq.gz files
IN="/home/dwragg/work/Project_ROYALBEE.301/Run_Pool1.4968/RawData/"
# Read in list of files
SAMPLELIST=(${IN}*)
# Strip out the suffix
tmp1=(${SAMPLELIST[@]/_R1.fastq.gz/})
tmp2=(${tmp1[@]/_R2.fastq.gz/})
# Strip out the prefix
tmp3=(${tmp2[@]/${IN}/})
# Reduce to unique list
tmp4=( $( printf "%s\n" "${tmp3[@]}" | awk 'x[$0]++ == 0' ) )
# Iteration would commence here, or a list for qarray
ID=${tmp4[2]}

DUMP="/home/dwragg/work/tmp"
PIPE="/home/dwragg/work/Pipeline"
#mkdir -p /home/dwragg/work/tmp/${ID}/logs
mkdir -p ${DUMP}/logs/${ID}
# Run from a root directory, eg ${DUMP}, to avoid risk of an SGE_O_WORKDIR error
cd ${DUMP}
qsub -q workq -l mem=8G -l h_vmem=8G -pe parallel_smp 8 -o ${DUMP}/logs/${ID} -e ${DUMP}/logs/${ID} ${PIPE}/snps.sh -i ${PIPE}/params -s ${ID} 

# JFM11-PCRfree aborted on stats during dcov for some unknown reason

# map.sh -> stat.sh -> snps.sh

# ==============================================================================
# Mapping Loop
# ==============================================================================

# subset array tmp4
tmp5=${tmp4[@]:4:22}

# Set root paths
DUMP="/home/dwragg/work/tmp"
PIPE="/home/dwragg/work/Pipeline"

# qsub loop
for ID in ${tmp5[@]}
do
  mkdir -p ${DUMP}/logs/${ID}
  cd ${DUMP}
  qsub -q unlimitq -l mem=8G -l h_vmem=8G -pe parallel_smp 8 -o ${DUMP}/logs/${ID} -e   ${DUMP}/logs/${ID} ${PIPE}/map.sh -i ${PIPE}/params -s ${ID} 
done




